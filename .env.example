# ============================================
# LLM Gateway - Environment Configuration
# ============================================
# Kopiere diese Datei nach .env und passe sie an:
#   cp .env.example .env

# ----- Server (PFLICHT) -----
# Geheimer API-Schlüssel für Gateway-Authentifizierung
GATEWAY_SECRET=dein-geheimer-schluessel-hier

# ----- Lokales LLM -----
# Aktiviere lokales LLM (Ollama, LM Studio, etc.)
LOCAL_LLM_ENABLED=true
# URL zum lokalen OpenAI-kompatiblen Endpunkt
# Ollama: http://localhost:11434/v1
# LM Studio: http://localhost:1234/v1
# Für Docker: http://host.docker.internal:11434/v1
LOCAL_LLM_URL=http://host.docker.internal:11434/v1
# API-Key (bei Ollama beliebig)
LOCAL_LLM_API_KEY=local
# Standard-Modell
LOCAL_LLM_MODEL=llama3.2:latest

# ----- Anthropic (Claude) -----
# Hol dir einen API-Key: https://console.anthropic.com
ANTHROPIC_API_KEY=
# Für 'claude setup-token': Token hier einfügen
# Führe 'claude setup-token' aus und kopiere den Token
ANTHROPIC_SETUP_TOKEN=

# ----- Groq (Optional) -----
# Für Cloud-Fallback wenn lokales LLM nicht verfügbar
GROQ_API_KEY=

# ----- OpenAI (Optional) -----
OPENAI_API_KEY=

# ----- Routing -----
# "local" = Routing-Entscheidungen via lokales LLM
# "groq" = Routing via Groq Cloud
ROUTER_PROVIDER=local

# ----- Budget Limits (USD pro Tag) -----
DAILY_BUDGET_SOFT=5.0
DAILY_BUDGET_MEDIUM=15.0
DAILY_BUDGET_HARD=50.0

# ----- Rate Limits -----
RATE_LIMIT_RPM=60
RATE_LIMIT_TPM=100000

# ----- Cache -----
SEMANTIC_THRESHOLD=0.92

# ----- Ports -----
GATEWAY_PORT=8000
DASHBOARD_PORT=3000

# ----- Grafana (nur mit --profile monitoring) -----
GRAFANA_PASSWORD=admin

# ============================================
# LLM Gateway Konfiguration v2.0
# ============================================
# Drei-Tier-Routing: LOCAL → CHEAP → PREMIUM
#
# LOCAL:   Kostenlos, schnell, für einfache Aufgaben
# CHEAP:   Günstig, für mittlere Komplexität  
# PREMIUM: Claude Sonnet, für komplexe Aufgaben

server:
  host: "0.0.0.0"
  port: 8000
  secret: "${GATEWAY_SECRET}"
  log_level: "INFO"

# ============================================
# Provider-Konfiguration (Drei Schichten)
# ============================================
providers:
  # ---- SCHICHT 1: LOCAL (Kostenlos) ----
  # Für: Einfache Erklärungen, Shell-Commands, Quick Fixes
  local:
    enabled: true
    base_url: "${LOCAL_LLM_URL:-http://host.docker.internal:8080/v1}"
    api_key: "${LOCAL_LLM_API_KEY:-local}"
    
    # Empfohlene Modelle für verschiedene Aufgaben:
    models:
      # Router/Classifier: Schnell, klein
      router: "GLM-4.7-Flash-UD-Q5_K_XL"
      
      # Einfache Anfragen: Schnell
      simple: "Llama-3.2-3B-Instruct-UD-Q4_K_XL"
      
      # Mittlere Anfragen: Besser
      medium: "qwen2.5-14b-instruct-q6_k"
      
      # Embeddings für Semantic Cache
      embedding: "nomic-embed-text-v1.5.Q8_0"
    
    # Standard-Modell für LOCAL-Tier
    default_model: "${LOCAL_LLM_MODEL:-GLM-4.7-Flash-UD-Q5_K_XL}"
    timeout: 120
    max_tokens: 4096
  
  # ---- SCHICHT 2: CHEAP (Optional, Cloud-Fallback) ----
  # Für: Code-Review, Refactoring, mittlere Komplexität
  # Kann auch lokal sein (größeres Modell)
  cheap:
    enabled: true
    # Lokal: Größeres Modell verwenden
    provider: "local"
    model: "qwen2.5-14b-instruct-q6_k"
    # Oder Cloud-Fallback:
    # provider: "groq"
    # model: "llama-3.1-8b-instant"
  
  # ---- SCHICHT 3: PREMIUM (Claude) ----
  # Für: Architektur, komplexes Debugging, Security
  anthropic:
    enabled: true
    api_key: "${ANTHROPIC_API_KEY:-}"
    setup_token: "${ANTHROPIC_SETUP_TOKEN:-}"
    default_model: "claude-sonnet-4-20250514"
    use_prompt_caching: true
    max_tokens: 8192
  
  # ---- Groq (Optional, Fallback) ----
  groq:
    enabled: false
    api_key: "${GROQ_API_KEY:-}"

# ============================================
# Routing-Konfiguration
# ============================================
routing:
  # Router verwendet lokales LLM
  router_provider: "local"
  router_model: "GLM-4.7-Flash-UD-Q5_K_XL"
  
  # Tier-Definitionen
  tiers:
    # LOCAL: Einfache Anfragen → GLM-4.7 oder Llama-3.2
    local:
      enabled: true
      provider: "local"
      model: "GLM-4.7-Flash-UD-Q5_K_XL"
      categories:
        - "simple_explanation"
        - "shell_command"
        - "quick_fix"
        - "documentation"
        - "translation"
      context_budget: 4000
    
    # CHEAP: Mittlere Anfragen → Qwen2.5-14B
    cheap:
      enabled: true
      provider: "local"
      model: "qwen2.5-14b-instruct-q6_k"
      categories:
        - "code_review"
        - "refactoring"
        - "testing"
        - "debugging_simple"
      context_budget: 8000
    
    # PREMIUM: Komplexe Anfragen → Claude
    premium:
      enabled: true
      provider: "anthropic"
      model: "claude-sonnet-4-20250514"
      categories:
        - "architecture"
        - "complex_debugging"
        - "security_review"
        - "optimization"
        - "multi_file_refactor"
      context_budget: 16000

# ============================================
# Caching
# ============================================
caching:
  data_dir: "/app/data"
  
  exact:
    enabled: true
    ttl: 604800  # 7 Tage
  
  semantic:
    enabled: true
    similarity_threshold: 0.92
    ttl: 259200  # 3 Tage
    # Lokale Embeddings mit nomic-embed-text
    embedding_provider: "local"
    embedding_model: "nomic-embed-text-v1.5.Q8_0"

# ============================================
# Budget & Rate-Limits
# ============================================
budget:
  daily:
    soft: 5.0
    medium: 15.0
    hard: 50.0

rate_limits:
  requests_per_minute: 60
  tokens_per_minute: 100000
